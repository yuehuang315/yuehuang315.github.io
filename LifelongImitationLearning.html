
<!-- Ref:http://vpg.cs.princeton.edu/ -->


<html class="no-js" lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Robot Grasping in Cluttered Environment with Active Exploration</title>
	<link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
	<link rel="icon" href="favicon.ico" type="image/x-icon">
    <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,900" rel="stylesheet">
    <link rel="stylesheet" href="libs/font-awesome/css/font-awesome.min.css">
    <!--??-->
    <link href="css/project.css" rel="stylesheet">
</head>

<body>

    <div id="main" style="padding-bottom:1em; padding-top: 5em; width: 60em; max-width: 70em; margin-left: auto; margin-right: auto;">
        <section id="four">
            <h1 style="text-align: center; margin-bottom: 0;">
                Lifelong Imitation Learning
            </h1>
            <h2 style="text-align: center;"> via Dynamic Movement Primitives</h2>
            <br>
            <section>
                <div class="box alt" style="margin-bottom: 1em;">
                    <h5 style="text-align: center;">Yue Huang<sup>*</sup>,  
                            Huaping Liu<sup>$</sup></h5>

                </div>
            </section>

            <!-- <div class="row 50% uniform" style="width: 80%;">
                    <div class="1u" style="font-size: 0.8em; line-height: 1.5em; text-align: center;">Yue Huang <sup>*</sup></div>
                    <div class="1u$" style="font-size: 0.8em; line-height: 1.5em; text-align: center;">Huaping Liu <sup>$</sup></div>
                </div> -->
            <h6 style="color: #a2a2a2; margin-bottom: 2em;">
                <sup>$</sup> Corresponding author: hpliu@tsinghua.edu.cn<br>
                All authers are from Department of Computer Science and Technology, Tsinghua University, Beijing, China
            </h6>

            <hr>
            <br>
            <b><h2 style="text-align: center;">Abstract</h2></b>

            <p>
              Imitation learning is an essential learning method
              to help robots to be adapted to different tasks. However, existing
              imitation learning work usually focuses on specific tasks but
              lacks the generative capability to new tasks. In this work,
              we establish a novel lifelong imitation learning framework
              to perform the sequential multi-task imitation learning and
              explicitly store the experience using a dictionary. To solve
              the action representation problem, the dynamic movement
              primitives are introduced to be seamlessly combined with the
              lifelong learning framework. Besides, an efficient incremental
              dictionary learning method is developed to store and update
              the operation skills. Finally, the proposed method is validated
              on a series of robotic manipulation tasks and shows promising
              results.
            </p>

            <div class="12u$"><a href=""><span class="image fit"><img src="images/lifelong_pic1.png" alt=""></span></a></div>
            
            <!-- Insert a video and image in the same row -->
            <!-- <div class="box alt" style="margin-bottom: 1em">
            </div> -->

            <p><i>Our grasping system</i> consists of a composite
                    robotic hand for grasping, a UR5 manipulator for reaching the operation
                    point. We introduce the strategy of
                    active exploration applied on the environment for more promising grasping.  
            </p>       
            <hr>

            <!-- about the paper link and the author infomation -->
            <!-- <p style="margin-bottom: 1em;">Latest version (27 Mar 2018): <a href="https://arxiv.org/abs/1803.09956">arXiv:1803.09956 [cs.RO]</a> or <a href="paper.pdf">here</a>.<br>To appear at IEEE International Conference on Intelligent Robots and Systems (IROS) 2018<br><font color="4e79a7">★ Best Cognitive Robotics Paper Award Finalist, IROS ★</font></p>
            <div class="12u$"><a href="https://arxiv.org/pdf/1803.09956.pdf"><span class="image fit" style="border: 1px solid; border-color: #888888;"><img src="images/paper-thumbnail.jpg" alt=""></span></a></div> -->

            <!-- <sup>1</sup> Princeton University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup> Google&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>3</sup> Massachusetts Institute of Technology -->

            <!-- about the code link and the bibtex infomation -->
            <!-- <div class="row">
                <div class="6u 12u$(xsmall)">
                    <h3>Code</h3>
                    Code is available on <a href="https://github.com/andyzeng/visual-pushing-grasping">Github</a>. Includes:
                    <ul>
                        <li>Training/testing code (with PyTorch/Python)</li>
                        <li>Simulation environments (with V-REP)</li>
                        <li>Code for real-world setups (with UR5 robots)</li>
                        <li>Pre-trained models and baselines</li>
                        <li>Evaluation code (with Python)</li>
                    </ul>
                </div>
                <div class="6u$ 12u$(xsmall)">
                    <h3>Bibtex</h3>
                    <pre><code>@article{zeng2018learning,
                    title={Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning},
                    author={Zeng, Andy and Song, Shuran and Welker, Stefan and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
                    journal={arXiv preprint arXiv:1803.09956},
                    year={2018}
                    }</code></pre>
                </div>
            </div> -->
            
            <b><h2 style="text-align: center;">Summary Video</h2></b>
            <div class="12u$">
                <video class="image fit" style="margin-bottom: 0.5em;" controls="" data-video="0">
                    <source src="images/RobotGrasping/main.mp4" type="video/mp4">Your browser does not support this video.
                    </video>
                </div>
            <hr>

            <hr>
            <b><h2 style="text-align: center;">Example Results</h2></b>
            <h3>Characteristics of Grasp Process</h3>
            <p>Compared with other suction grasping systems, 
                the proposed composite robotic hand uses the two fingers to hold the
                object after the suction cup lifts the object, which increases the stability of the grasp:
            </p>

            <div class="box alt">
                <div class="row 50% uniform">
                    <div class="6u"><video class="image fit" style="margin-bottom: 0.5em;" controls="" data-video="1">
                        <source src="images/RobotGrasping/grasp1.mp4" type="video/mp4">Your browser does not support this video.</video>
                        <!-- <h5 style="color: #a2a2a2; margin-bottom: 1em;">Total # of actions: 7 (task complete)</h5> -->
                    </div>
                    <div class="6u$"><video class="image fit" style="margin-bottom: 0.5em;" controls="" data-video="2">
                        <source src="images/RobotGrasping/grasp2.mp4" type="video/mp4">Your browser does not support this video.</video>
                        <!-- <h5 style="color: #a2a2a2; margin-bottom: 1em;">Total # of actions: 7 (task complete)</h5> -->
                    </div>            
                </div>
            </div>

            <!-- <h6 style="color: #a2a2a2; margin-bottom: 2em;">Note:</h6> -->

            <h3>Robotic Experiments</h3>
            <p>We test our DQN model on real environment, including a Microsoft’s Kinect V2 camera as the image acquisition tool
                to get the RGB image and depth image of the scene and a UR5 manipulator to carry our composite robotic hand.
                We select 40 different objects to build different scenes for our robotic hand to grasp.
            </p>

            <!-- Add a container including four videos in it -->

            <!-- <p style="margin-bottom: 1em;">For more quantitative evaluations and ablation studies (in both simulation and real-world settings), please check out our <a href="https://arxiv.org/abs/1803.09956">technical report</a>. There, we also explore some interesting questions like:
            </p>
            <ul>
                <li>Is it possible to train pushing policies without any rewards? Can intrinsic rewards help?</li>
                <li>Does long-term lookahead matter for planning VPG strategies in picking?</li>
                <li>Is it possible to train VPG policies without ImageNet pre-training? How much do pre-trained weights influence sample complexity and performance?</li>
                <li>Can we train VPG policies with only color information (no depth/height-from-bottom information)?</li>
            </ul> -->

            <!-- <h4>Failure Modes</h4> -->

            <hr>
            <b><h3>Contact</h3></b>
            <p>Have any questions, please feel free to contact <a href="https://yuehuang315.github.io/">Yue Huang</a></p>
            <hr>
        </section>
    </div>

    <!-- Copyright -->
    <!-- <footer id="footer">
            <div class="inner">
                <ul class="copyright">
                    <p>Copyright &copy; 2019 Yue Huang</p>
                </ul>
            </div>
        </footer> -->

    <script src="js/project/main.js"></script>
    <script src="js/project/util.js"></script>
    <script src="js/project/skel.min.js"></script>
    <script src="js/project/jquery.min.js"></script>
    <script src="js/project/jquery.poptrox.min.js"></script>
</body>

</html>

    © 2019 GitHub, Inc.
    Terms
    Privacy
    Security
    Status
    Help

    Contact GitHub
    Pricing
    API
    Training
    Blog
    About

